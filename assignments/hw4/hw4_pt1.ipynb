{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_F_jb0mnswh"
   },
   "source": [
    "# CIS 1902 Homework 4 Part 1: Deep LearningðŸ‘• ðŸ‘  ðŸ‘— ðŸ‘Ÿ ðŸ§¥\n",
    "\n",
    "**Due Friday March 31, 2023 11:59 pm EST**\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Practice using PyTorch\n",
    "- Familiarization with deep learning model building and tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y43Ou7afUZMG"
   },
   "source": [
    "**First, make a copy of this Colab to your Google Drive by clicking \"Copy to Drive\" in the upper left or `File -> save a copy in Drive` so you can save any changes you make.**\n",
    "\n",
    "- **Name:** TODO\n",
    "- **PennKey:** TODO\n",
    "- **Number of hours spent on homework** \n",
    "    - **Part 1:** TODO\n",
    "\n",
    "Collaboration is NOT permitted.\n",
    "\n",
    "In the functions below the \"NotImplementedError\" exception is raised, for\n",
    "you to fill in. The interpreter will not consider the empty code blocks\n",
    "as syntax errors, but the \"NotImplementedError\" will be raised if you\n",
    "call the function. You will replace these raised exceptions with your\n",
    "code completing the function as described in the docstrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RazaloHpvK9V"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Installing PyTorch or any other deep learning framework can be a huge hassle on your local machine, especially if you're trying to take advantage of GPU acceleration. Thankfully Colab has all of this taken care of for us: PyTorch comes installed by default, and Google even provides free GPU resources for us to use.\n",
    "\n",
    "To enable GPU acceleration for your notebook, go to the menu and `Runtime -> Change Runtime type`. Under `Hardware accelerator`, choose `GPU`.\n",
    "\n",
    "Confirm that you have a GPU allocated by importing `torch` and running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Fj6X13Nwp0y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFwfYStT0b1u"
   },
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9NMcfTaw543"
   },
   "source": [
    "If the above command returns 1, that means Colab has allocated a GPU for your session, and so you should be good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn3DzwMus348"
   },
   "source": [
    "## The Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uYiGsF4xHKT"
   },
   "source": [
    "We looked at the MNIST dataset in class, which is the \"Hello world!\" dataset of sorts for image classification. However, with deep learning becoming so *fashionable* these days, the old MNIST classification task has become too easy, with many simple architectures achieving well above 99% accuracy. \n",
    "\n",
    "Enter the [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which is intended as a drop-in replacement to MNIST.\n",
    "\n",
    " ![](https://www.seas.upenn.edu/~cis1920/tliu/s23/hws/hw5/fashion-mnist-annotated.png)\n",
    "\n",
    "There are still 10 classes to classify and the inputs are still 28 by 28 pixel grayscale images, but instead of handwritten digits there are articles of clothing, which is a harder classification task. We will be using Fashion-MNIST to practice using PyTorch as well as explore the full machine learning process of model training and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnKIILoMd2YJ"
   },
   "source": [
    "### Loading and processing the data\n",
    "\n",
    "We first need to load and process the data. We've provided the `load_data()` function below, which downloads and loads the Fashion MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzN9FFnqxGrH"
   },
   "outputs": [],
   "source": [
    "def load_data(batch_size=64):\n",
    "    \"\"\"\n",
    "    Load the fashion MNIST data using torchvision and apply the appropriate \n",
    "    transformations.\n",
    "    \n",
    "    transforms.Normalize manipulates the input such that:\n",
    "    new_input = (old_input - mean) / std\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): the batch size for training and testing\n",
    "        mean (float): the mean to normalize images with\n",
    "        std (float): the std to normalize images with\n",
    "\n",
    "    Returns:\n",
    "        trainloader (torch.utils.DataLoader): data loader for the train set\n",
    "        validloader (torch.utils.DataLoader): data loader for the validation set\n",
    "    \"\"\"\n",
    "    # Normalize the data to [-1, 1]\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=0.5, std=0.5)])\n",
    "    \n",
    "    # Download and load data\n",
    "    fmnist = datasets.FashionMNIST('pytorch/', download=True, train=True, \n",
    "                                     transform=transform)\n",
    "    \n",
    "    # split the data into training and validation, set seed for reproducibility\n",
    "    rand_seed = torch.Generator().manual_seed(42)\n",
    "    trainset, validset = torch.utils.data.random_split(fmnist, [30000, 30000], \n",
    "                                                       generator=rand_seed)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                              shuffle=True)\n",
    "\n",
    "    validloader = torch.utils.data.DataLoader(validset, batch_size=batch_size, \n",
    "                                             shuffle=True)\n",
    "\n",
    "    return trainloader, validloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NchSHTwRkc5F"
   },
   "source": [
    "### Visualizing the examples\n",
    "\n",
    "As we saw in class, we can iterate over `torch.DataLoaders`, which return a `(images, labels)` tuple at each iteration. Let's use the data loaders returned from `load_data()` to explore what the classes look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eDUZyD3NTjy"
   },
   "outputs": [],
   "source": [
    "img_dict = {0: 'T-shirt/top',\n",
    "           1: 'Trouser',\n",
    "           2: 'Pullover',\n",
    "           3: 'Dress',\n",
    "           4: 'Coat',\n",
    "           5: 'Sandal',\n",
    "           6: 'Shirt',\n",
    "           7: 'Sneaker',\n",
    "           8: 'Bag',\n",
    "           9: 'Ankle boot'}\n",
    "\n",
    "def show_random_fmnist():\n",
    "    \"\"\"\n",
    "    Displays random fashion-MNIST examples.\n",
    "    \"\"\"\n",
    "    num_examples = 10\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    trainloader, _ = load_data(batch_size=num_examples)\n",
    "\n",
    "    # grab random images and labels\n",
    "    # shape (num_examples, 28, 28)\n",
    "    images, labels = next(iter(trainloader))\n",
    "    \n",
    "    # reshape to a grid shape\n",
    "    images = images.reshape(num_examples*28,1*28)\n",
    "\n",
    "    # imshow displays pixel images\n",
    "    ax.imshow(images, cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "\n",
    "    custom_ticks = np.arange(14, 14 + (28 * num_examples), 28)\n",
    "    ax.set_yticks(custom_ticks)\n",
    "    img_classes = [img_dict[label] for label in labels.numpy()]\n",
    "    \n",
    "    ax.set_yticklabels(img_classes)\n",
    "    ax.set_title('{} random fashion MNIST examples'.format(num_examples))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "show_random_fmnist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icvomcQy0Zl-"
   },
   "source": [
    "## Building neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QjEx0LSsdCV"
   },
   "source": [
    "### Fully connected network [1 pt]\n",
    "\n",
    "Let's begin by creating a fully connected neural network in `create_fnn(input_size, num_classes)`. Your implementation should have:\n",
    "\n",
    "- two `nn.Linear` hidden layers, each with 64 neurons\n",
    "- two `nn.ReLU` activations after each hidden layer\n",
    "- a final `nn.Linear` layer for the output\n",
    "\n",
    "**Note:** our autograder will check against this **exact** architecture, so please follow these instructions exactly. \n",
    "\n",
    "### Your custom architecture\n",
    "\n",
    "Once you have gotten our specified architecture to work, feel free to build your own custom architecture to achieve better test accuracy. For example, you could increase the number of layers, increase the number of neurons per layer,\n",
    "or experiment with different modules such as [`nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) for your network.\n",
    "\n",
    "**We recommend coming back to `create_custom_nn` after you have implemented the other functions to compute the accuracy and train your networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCeItJao0BCl"
   },
   "outputs": [],
   "source": [
    "def create_fnn(in_size, num_classes):\n",
    "    \"\"\"\n",
    "    Create a fully connected neural network as specified above: two hidden \n",
    "    linear layers with 64 units, each followed by ReLU activation.\n",
    "\n",
    "    Note: there are 2 *hidden* layers. This means there must\n",
    "    be an additional third layer for the output.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): the input size of the network, should match data shape\n",
    "        num_classes (int): the # of classes, which is the network output size\n",
    "\n",
    "    Returns:\n",
    "        fnn (nn.Sequential): the fully connected network whose input is of\n",
    "        size in_size and output is of size num_classes\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    fnn = nn.Sequential(\n",
    "        # Flatten needed as the images are originally 2D\n",
    "        nn.Flatten(),\n",
    "        # TODO fill in the rest\n",
    "    )\n",
    "\n",
    "    return fnn\n",
    "\n",
    "def create_custom_nn(in_size, num_classes):\n",
    "    \"\"\"\n",
    "    Create a custom neural network of your choosing. Feel free to copy your\n",
    "    implementation of either create_fnn or create_cnn and modify it here.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): the input size of the network, should match data shape\n",
    "        num_classes (int): the # of classes, which is the network output size\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: a nn Sequential representing your neural network.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7SPl7FX0eug"
   },
   "source": [
    "## Training and evaluating neural networks\n",
    "\n",
    "Once you have built your neural net architectures, we'll need to train them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_rpqZ-KslRK"
   },
   "source": [
    "### Calculating accuracy [1 pt]\n",
    "\n",
    "First implement the `compute_accuracy()` function, which computes the accuracy of a given `net` on the given `dataloader` data.\n",
    "\n",
    "For an n x 784 input x, net(x) gives an n x 10 output. For each row i in the output, the 10 columns are proportional to the probability of each class. To find the prediction, you must find the index of the column with the highest probability for each row. You can use `torch.argmax()` to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh7xQfWJ1FCZ"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(net, dataloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Return the accuracy of the network on all data points in the dataloader. \n",
    "    This is the sum of the number of correct predictions divided by the total \n",
    "    number of samples in the dataloader.\n",
    "\n",
    "    Hint: use torch.argmax() to find the element with the highest probability.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        net (nn.Sequential): the network to compute the accuracy of\n",
    "        dataloader (utils.DataLoader): a dataloader to compute accuracy over\n",
    "        device (str): the device to send Tensors to\n",
    "    Returns:\n",
    "        float: the net's accuracy on the data from dataloader\n",
    "    \"\"\"\n",
    "    # no_grad speeds up computation by telling PyTorch not to compute gradients\n",
    "    with torch.no_grad():\n",
    "        tot_correct = 0\n",
    "        tot_samples = 0\n",
    "        for im, lab in dataloader:\n",
    "            imgs, labels = im.to(device), lab.to(device)\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO fill out:\n",
    "                1. get y_hat predictions by calling net(imgs)\n",
    "                2. add the number of correct predictions made for the batch\n",
    "                3. add the number of samples seen in the batch\n",
    "            \"\"\"\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return (tot_correct / tot_samples).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot9FsMSj0I-X"
   },
   "source": [
    "\n",
    "### Training loop [1 pt]\n",
    "\n",
    "Next, fill out the  `train_nn()` functions, following the steps we discussed in class:\n",
    "\n",
    "- For each epoch:\n",
    "    - For each minibatch:\n",
    "        1. zero the gradients\n",
    "        2. compute the output predictions\n",
    "        3. compute the loss\n",
    "        4. backpropagate\n",
    "        5. use the optimizer to take a step\n",
    "\n",
    "Refer to the lecture recording for the corresponding Pytorch commands for each step. We provide the loss function `nn.CrossEntropyLoss()` and optimizer `optim.Adam()` as shown in lecture, but feel free to experiment with other options.\n",
    "\n",
    "We also provide a `create_acc_curve()` function to plot the training and testing performance of your neural network over the training iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epcB_Csg0oWd"
   },
   "outputs": [],
   "source": [
    "def train_nn(net, trainloader, validloader, eval_freq, num_epochs, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Train the network net on data from the trainloader.\n",
    "    Recall the high-level algorithm:\n",
    "    For each epoch\n",
    "        For each minibatch\n",
    "            zero the gradients\n",
    "            compute the output predictions\n",
    "            compute the loss\n",
    "            backpropagate\n",
    "            use the optimizer to take a step\n",
    "    Args:\n",
    "        net (nn.Sequential): a neural network\n",
    "        trainloader (DataLoader): the data loader for training set\n",
    "        validloader (DataLoader): the data loader for the validation set\n",
    "        eval_freq (int): the frequency at which to compute the train/valid acc\n",
    "        num_epochs (int): the number of epochs (or complete passes) over the \n",
    "                          train data\n",
    "        device (str): the device to train on, either \"cpu\" or \"cuda\"\n",
    "\n",
    "    ret:\n",
    "        train_acc: the accuracy over the training set  computed every eval_freq \n",
    "            iterations\n",
    "        valid_acc: the accuracy over the validation set \n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "    iter_num = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for imgs, labels in trainloader:\n",
    "            imgs_train, labels_train = imgs.to(device), labels.to(device)\n",
    "\n",
    "            # TODO: fill out the steps needed, as outlined above\n",
    "            \n",
    "\n",
    "            \n",
    "            # End TODO\n",
    "\n",
    "            if iter_num % eval_freq == 0:\n",
    "                print(\"Iteration: {}\".format(iter_num))\n",
    "                train_acc.append(compute_accuracy(net, trainloader, device))\n",
    "                valid_acc.append(compute_accuracy(net, validloader, device))\n",
    "            iter_num += 1\n",
    "    return train_acc, valid_acc\n",
    "    \n",
    "\n",
    "def create_acc_curve(train_acc, valid_acc, eval_freq):\n",
    "    \"\"\"\n",
    "    Create a accuracy curve plot for both the test and train data.\n",
    "\n",
    "    Args:\n",
    "        train_acc (list): a list of the training accuracy over time, one for \n",
    "            each eval_freq iterations\n",
    "        valid_acc (list): a list of the validation accuracy over time, one for \n",
    "            each eval_freq iterations\n",
    "        eval_freq (int): the number of iterations between each accu measurement\n",
    "    \n",
    "    Returns: \n",
    "        None, but writes to acc_curves.png\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.figure()\n",
    "    x_axis = np.arange(0, len(train_acc) * eval_freq, eval_freq)\n",
    "    ax.plot(x_axis, train_acc, \n",
    "            label='Train final acc: {:.3f}'.format(train_acc[-1]))\n",
    "    ax.plot(x_axis, valid_acc, \n",
    "            label='Validation final acc: {:.3f}'.format(valid_acc[-1]))\n",
    "    ax.set_title('Accuracy on Fashion-MNIST')\n",
    "    ax.set_ylabel('accuracy')\n",
    "    ax.set_xlabel('# iterations')\n",
    "    ax.legend()\n",
    "    fig.savefig(\"acc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-SEX0w0sQ-"
   },
   "source": [
    "## The model training process\n",
    "\n",
    "Once both `train_nn()` and `compute_accuracy()` are implemented, you are ready to start training your network. Fill out and run the code in the `__main__` function below. On a GPU, this should take about 2 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gq0cd4Zi0OM9"
   },
   "outputs": [],
   "source": [
    "# Note: the fully connected net will take about 2 minutes per epoch to train!\n",
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    device = \"cuda\"\n",
    "    # TODO this should be the total number of pixels in an image\n",
    "    input_size = None\n",
    "    # TODO this should be the total number of FMNIST classes\n",
    "    num_classes = None\n",
    "    \n",
    "    \n",
    "    eval_freq = 100\n",
    "    # TODO tune the number of epochs for better performance.\n",
    "    num_epochs = 1\n",
    "    batch_size = 64\n",
    "    trainloader, validloader = load_data(batch_size=batch_size)\n",
    "\n",
    "    nn_model = create_fnn(input_size, num_classes)\n",
    "    # uncomment to experiment with your custom neural net\n",
    "    # nn_model = create_custom_nn(input_size, num_classes)\n",
    "    \n",
    "    train_acc, valid_acc = train_nn(nn_model, \n",
    "                                   trainloader, \n",
    "                                   validloader, \n",
    "                                   eval_freq, \n",
    "                                   num_epochs, \n",
    "                                   device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yq99_5g_5liV"
   },
   "source": [
    "### Visualize the training curves \n",
    "\n",
    "After your network has been trained, you can visualize the training curves and print the final accuracies by running the `create_acc_curve()` method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvchIHEAt3P4"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_acc_curve(train_acc, valid_acc, eval_freq);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxvv0hvsTq1x"
   },
   "source": [
    "### Tuning your networks\n",
    "\n",
    "Using the neural network created in `create_fnn()` as a base, tune your model implementation in `create_custom_nn()` to maximize validation accuracy. Some things you can experiment with:\n",
    "\n",
    "- increasing the number of epochs\n",
    "- adding more linear layers, or changing the number of neurons per layer\n",
    "- adding dropout or convolutional layers (warning: may take quite a bit more tinkering than the other options)\n",
    "\n",
    "Once you are done tuning your model, run the code below to save your model to `model.pkl` and include it as part of your final submission. \n",
    "\n",
    "**NOTE: the Google Colab runtime will sometimes time out and reset, so be sure to save a copy of your model.pkl file immediately to your local machine as a backup.**\n",
    "\n",
    "Download and include the `acc_curves.png` plot for your final network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulqDMm3saORQ"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.save(nn_model, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEwgksv9NQGk"
   },
   "source": [
    "### Final model test accuracy [1 pt]\n",
    "\n",
    "In real-world machine learning situations, we have at least **three** data splits:\n",
    "\n",
    "- **Training data**: Data that is used to train and optimize model parameters\n",
    "- **Validation data**: Data that is used for tuning the model (choosing hyperparameters like model architecture, batch size, etc).\n",
    "- **Testing data**: Data used to evaluate the final performance of the model, that is held out separately from the training and validation process.\n",
    "\n",
    "A very common pitfall is to use a portion of the dataset for both validation and testing. This is **extremely bad practice**, and it will often lead to model overfitting.\n",
    "\n",
    "Thus, the data the autograder will evaluate your neural network will be a completely unseen test set. This mirrors the set up of real-world machine learning competitions such as those run by Kaggle, where competitors are given a set of the data to train and validate their models against, but their leaderboard ranking is based on a held-out test set that they do not have access to.\n",
    "\n",
    "You will receive full credit if your model achieves >83% accuracy on the **testing data.** Like all the other homework assignments, you are free to submit to Gradescope as many times without penalty, so you can experiment with model architectures to see what produces the best results. Generally, the validation accuracy is a decent approximation of the test accuracy, so you can use that as a guide.\n",
    "\n",
    "You can get **1 extra credit point** on this assignment if your model achieves >89%\n",
    "accuracy.\n",
    "\n",
    "> **TIP**: if implemented correctly, the fully connected network from `create_fnn()` should be able to achieve this performance just by increasing the number of training epochs -- you won't need to implement `create_custom_nn()`. If you'd like to explore different deep learning architectures to try to get the extra credit point, we recommend looking into [convolutional neural networks](https://cs231n.github.io/convolutional-networks/), which have convenient layer implementations in PyTorch: [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) and [`nn.MaxPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#maxpool2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhcEe1tqp6o_"
   },
   "source": [
    "## Part 1 discussion [1 pt]\n",
    "\n",
    "**Write your response directly in this markdown cell.**\n",
    "\n",
    "1. Describe what you observe in your `acc_curves.png` plot. Does your neural network appear to \"learn\" (in terms of better performance) quickly or slowly? Do you see any differences between training accuracy, validation accuracy, and the autograder-reported testing accuracy?\n",
    "\n",
    "**Your response:** \n",
    "\n",
    "1. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANm6OlYEu9bQ"
   },
   "source": [
    "# Rubric\n",
    "\n",
    "| Sections | Points |\n",
    "|---------|--------|\n",
    "| **Part 1** |\n",
    "`create_fnn()` | 1\n",
    "`train_nn()` | 1\n",
    "`compute_accuracy()` | 1\n",
    "`acc_curves.png`, `model.pkl` uploaded | 1\n",
    "Achieve >83% test accuracy | 1\n",
    "Part 1 discussion | 1\n",
    "\n",
    "---\n",
    "\n",
    "| All Sections | Points |\n",
    "|---------|--------|\n",
    "All functions implemented | 0.5\n",
    "Pennkey, name, and hours estimate | 0.5\n",
    "Part 1 | 6\n",
    "Part 2 | 3\n",
    " **Total** | 10\n",
    "\n",
    "## Extra credit\n",
    "\n",
    "| Section | Points |\n",
    "|---------|--------|\n",
    "achieve >89% test accuracy | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUSjYzje6LGv"
   },
   "source": [
    "# Submission\n",
    "\n",
    "Afer you have completed the worksheet, download the Colab notebook as a `hw5_dl.py` file by going to `File -> Download .py`. Then submit your `hw5_dl.py`, `model.pkl`, and `acc_curves.png` files to Gradescope. \n",
    "\n",
    "Since this is only part 1 of the assignment, there will be points missing from the autograder -- the maximum autograder score for part 1 (not including the extra credit) is **5 points**.\n",
    "\n",
    "**Note: since you will be submitting Colab-formatted .py files, we will not be grading code style for this assignment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYhlzwO1P_e8"
   },
   "source": [
    "# Attribution\n",
    "\n",
    "Elements of this homework were adapted from [Kevin McGuinness](http://www.eeng.dcu.ie/~mcguinne/)' \"Introduction to Deep Learning for Computer Vision using PyTorch\" as well as the Fall 2019 CIS 520 convolutional neural net homework assignment."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOjxrd9p5VB3ahFWDkb9lj/",
   "provenance": [
    {
     "file_id": "1QfCPgQCYClqBWqyen03mWPg94Vq3nG5o",
     "timestamp": 1678816167999
    },
    {
     "file_id": "1v7_Q6rkHsnf-Oe7UQUO-osJ5R_PXUjQ3",
     "timestamp": 1678418890998
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
